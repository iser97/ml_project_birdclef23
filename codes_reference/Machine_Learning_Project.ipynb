{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVA7tH4OJHHM"
      },
      "source": [
        "# Prepare (import library and prepare data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4Zb5VsyJQXL"
      },
      "outputs": [],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "mRFgMA6VJJys",
        "outputId": "9d8388d4-f3a7-4c37-b90e-f051e626b52e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-245538b17de1>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import sys\n",
        "\n",
        "pd.options.mode.chained_assignment = None # avoids assignment warning\n",
        "import random\n",
        "from glob import glob\n",
        "tqdm.pandas()  # enable progress bars in pandas operations\n",
        "import gc\n",
        "import cv2\n",
        "import librosa\n",
        "import sklearn\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "# Import for visualization\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display as lid\n",
        "import IPython.display as ipd\n",
        "\n",
        "# from kaggle_datasets import KaggleDatasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "\n",
        "from transformers import set_seed\n",
        "from transformers import ASTFeatureExtractor\n",
        "from transformers import ASTPreTrainedModel, ASTModel, AutoConfig, ASTConfig\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwXSJ3M-JWTy"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln9Al8UHJXwz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "class CFG:\n",
        "    # Debugging\n",
        "    debug = False\n",
        "    \n",
        "    # Plot training history\n",
        "    training_plot = True\n",
        "    \n",
        "    # Weights and Biases logging\n",
        "    wandb = True\n",
        "    competition   = 'birdclef-2023' \n",
        "    _wandb_kernel = 'awsaf49'\n",
        "    \n",
        "    # Experiment name and comment\n",
        "    exp_name = 'baseline-v2'\n",
        "    comment = 'EfficientNetB0|FSR|t=10s|128x384|up_thr=50|cv_filter'\n",
        "    \n",
        "    # Notebook link\n",
        "    notebook_link = 'https://www.kaggle.com/awsaf49/birdclef23-effnet-fsr-cutmixup-train/edit'\n",
        "    \n",
        "    # Verbosity level\n",
        "    verbose = 0\n",
        "    \n",
        "    # Device and random seed\n",
        "    device = 'TPU-VM'\n",
        "    seed = 42\n",
        "    \n",
        "    # Input image size and batch size\n",
        "    img_size = [128, 384]\n",
        "    batch_size = 32\n",
        "    upsample_thr = 50 # min sample of each class (upsample)\n",
        "    cv_filter = True # always keeps low sample data in train\n",
        "    \n",
        "    # Inference batch size, test time augmentation, and drop remainder\n",
        "    infer_bs = 2\n",
        "    tta = 1\n",
        "    drop_remainder = True\n",
        "    \n",
        "    # Number of epochs, model name, and number of folds\n",
        "    epochs = 25\n",
        "    model_name = 'EfficientNetB0'\n",
        "    fsr = True # reduce stride of stem block\n",
        "    num_fold = 5\n",
        "    \n",
        "    # Selected folds for training and evaluation\n",
        "    selected_folds = [0]\n",
        "\n",
        "    # Pretraining, neck features, and final activation function\n",
        "    pretrain = 'imagenet'\n",
        "    neck_features = 0\n",
        "    final_act = 'softmax'\n",
        "    \n",
        "    # Learning rate, optimizer, and scheduler\n",
        "    lr = 1e-3\n",
        "    scheduler = 'cos'\n",
        "    optimizer = 'Adam' # AdamW, Adam\n",
        "    \n",
        "    # Loss function and label smoothing\n",
        "    loss = 'BCE' # BCE, CCE\n",
        "    label_smoothing = 0.05 # label smoothing\n",
        "    \n",
        "    # Audio duration, sample rate, and length\n",
        "    duration = 10 # second\n",
        "    sample_rate = 32000\n",
        "    target_rate = 8000\n",
        "    audio_len = duration*sample_rate\n",
        "    \n",
        "    # STFT parameters\n",
        "    nfft = 2048\n",
        "    window = 2048\n",
        "    hop_length = audio_len // (img_size[1] - 1)\n",
        "    fmin = 20\n",
        "    fmax = 16000\n",
        "    normalize = True\n",
        "    \n",
        "    # Data augmentation parameters\n",
        "    augment=True\n",
        "    \n",
        "    # Spec augment\n",
        "    spec_augment_prob = 0.80\n",
        "    \n",
        "    mixup_prob = 0.65\n",
        "    mixup_alpha = 0.5\n",
        "    \n",
        "    cutmix_prob = 0.0\n",
        "    cutmix_alpha = 0.5\n",
        "    \n",
        "    mask_prob = 0.65\n",
        "    freq_mask = 20\n",
        "    time_mask = 30\n",
        "\n",
        "\n",
        "    # Audio Augmentation Settings\n",
        "    audio_augment_prob = 0.5\n",
        "    \n",
        "    timeshift_prob = 0.0\n",
        "    \n",
        "    gn_prob = 0.35\n",
        "\n",
        "    # Data Preprocessing Settings\n",
        "    base_path = '/kaggle/input/birdclef-2023'  # for server: base_path = '/data/zjh_data/program/ml_project_birdclef23/birdclef-2023'\n",
        "    if not os.path.exists(base_path):\n",
        "        base_path = '/data/zjh_data/program/ml_project_birdclef23/birdclef-2023'\n",
        "    class_names = sorted(os.listdir('{}/train_audio'.format(base_path)))\n",
        "    num_classes = len(class_names)\n",
        "    class_labels = list(range(num_classes))\n",
        "    label2name = dict(zip(class_labels, class_names))\n",
        "    name2label = {v:k for k,v in label2name.items()}\n",
        "\n",
        "    # Training Settings\n",
        "    target_col = ['target']\n",
        "    tab_cols = ['filename']\n",
        "    monitor = 'auc'\n",
        "    \n",
        "    ### add by plathzheng\n",
        "    unilm_model_path = './pretrained_models/unilm/BEATs_iter3_plus_AS2M.pt'\n",
        "    use_apex = True\n",
        "    time_length = 10 # beats模型中，训练时，截取的音频片段时长\n",
        "    ast_fix_layer = 3 # the parameters in layer<ast_fix_layer would be fixed, choosen from [0, 5], if ast_fix_layer>5 all param woudl be fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJrSflVVJcq9"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "set_seed(CFG.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkx-eRRuJd1z"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "if CFG.debug:\n",
        "    device = torch.device('cpu')\n",
        "    CFG.use_apex = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH1CNCnPJetE"
      },
      "source": [
        "# Data Analyze and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm-NsomFJgek"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = '/kaggle/input/birdclef-2023'\n",
        "GCS_PATH = BASE_PATH\n",
        "\n",
        "test_paths = glob('/kaggle/input/birdclef-2023/test_soundscapes/*ogg')\n",
        "test_df = pd.DataFrame(test_paths, columns=['filepath'])\n",
        "test_df['filename'] = test_df.filepath.map(lambda x: x.split('/')[-1].replace('.ogg',''))\n",
        "\n",
        "df = pd.read_csv(f'{CFG.base_path}/train_metadata.csv')\n",
        "df['filepath'] = GCS_PATH + '/train_audio/' + df.filename\n",
        "df['target'] = df.primary_label.map(CFG.name2label)\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8yHGW9AJijh"
      },
      "source": [
        "## Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NATEGNKSJkIc"
      },
      "outputs": [],
      "source": [
        "def load_audio(filepath):\n",
        "    audio, sr = librosa.load(filepath)\n",
        "    return audio, sr\n",
        "\n",
        "def get_spectrogram(audio):\n",
        "    spec = librosa.feature.melspectrogram(y=audio, sr=CFG.sample_rate, \n",
        "                                   n_mels=CFG.img_size[0],\n",
        "                                   n_fft=CFG.nfft,\n",
        "                                   hop_length=CFG.hop_length,\n",
        "                                   fmax=CFG.fmax,\n",
        "                                   fmin=CFG.fmin,\n",
        "                                   )\n",
        "    spec = librosa.power_to_db(spec, ref=np.max)\n",
        "    return spec\n",
        "\n",
        "def display_audio(row):\n",
        "    # Caption for viz\n",
        "    caption = f'Id: {row.filename} | Name: {row.common_name} | Sci.Name: {row.scientific_name} | Rating: {row.rating}'\n",
        "    # Read audio file\n",
        "    audio, sr = load_audio(row.filepath)\n",
        "    # Keep fixed length audio\n",
        "    audio = audio[:CFG.audio_len]\n",
        "    # Spectrogram from audio\n",
        "    spec = get_spectrogram(audio)\n",
        "    # Display audio\n",
        "    print(\"# Audio:\")\n",
        "    display(ipd.Audio(audio, rate=CFG.sample_rate))\n",
        "    # print(\"# Image:\")\n",
        "    # show_image(row.common_name)\n",
        "    print('# Visualization:')\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12, 2*3), sharex=True, tight_layout=True)\n",
        "    fig.suptitle(caption)\n",
        "    # Waveplot\n",
        "    lid.waveshow(audio,\n",
        "                 sr=CFG.sample_rate,\n",
        "                 ax=ax[0])\n",
        "    # Specplot\n",
        "    lid.specshow(spec, \n",
        "                 sr = CFG.sample_rate, \n",
        "                 hop_length = CFG.hop_length,\n",
        "                 n_fft=CFG.nfft,\n",
        "                 fmin=CFG.fmin,\n",
        "                 fmax=CFG.fmax,\n",
        "                 x_axis = 'time', \n",
        "                 y_axis = 'mel',\n",
        "                 cmap = 'coolwarm',\n",
        "                 ax=ax[1])\n",
        "    ax[0].set_xlabel('');\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Lx57QhJnM6"
      },
      "source": [
        "## Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiyEDzzCJpxy"
      },
      "outputs": [],
      "source": [
        "stat = df.primary_label.value_counts().index.tolist()\n",
        "class_names = stat[:3] + stat[-3:] # popular + not popular\n",
        "print(class_names)\n",
        "\n",
        "class_name = class_names[0]\n",
        "print(f'# Category: {class_name}')\n",
        "class_df = df.query(\"primary_label==@class_name\")\n",
        "print(f'# Num Samples: {len(class_df)}')\n",
        "row = class_df.sample(1).squeeze()\n",
        "\n",
        "# Display audio\n",
        "display_audio(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0eUF3pCKNcN"
      },
      "source": [
        "## The overview of all data\n",
        "\n",
        "data class statistics, data class unbalanced figure, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "XHm1ZmXKKPdd",
        "outputId": "d9dac5f5-ca6e-4095-905d-0433cc9328e5"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3d8eed20150a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/birdclef-2023/train_metadata.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/birdclef-2023/train_metadata.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(\"/kaggle/input/birdclef-2023/train_metadata.csv\",engine='python')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7aH1SaXLN9A"
      },
      "source": [
        "## Visualization\n",
        "1. show a bird image in dataset (plan to do by zjh)\n",
        "1. feature visualization: data audio wav show, data spectrogram show, data mfcc show, data cross zero rate feature show (zjh)\n",
        "2. statistic (duration, average mfcc distribution, average amplitude distribution)  (Yaggy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_umsNj9M_NQ"
      },
      "source": [
        "### 1. bird image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnWSoByeMv_F"
      },
      "source": [
        "### 2. feature visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrE9kAHbMqxW"
      },
      "source": [
        "### 3. statistics (Yaggy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR-UeFSpJrrp"
      },
      "source": [
        "Load the all data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvLGQJPeJsDH"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/home/yangya/桌面/project/train_metadata.csv/train_metadata.csv\",engine='python')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1ohkr9mJ0j5"
      },
      "outputs": [],
      "source": [
        "all_class_labels=list(data.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1gLXc_DJ5dx"
      },
      "source": [
        "### Visulaize the distribution of each species (pie chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el75hLI0J-JD"
      },
      "outputs": [],
      "source": [
        "datagroup = data.groupby(\"primary_label\").count()\n",
        "datagroup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcYfdu-BKC_P"
      },
      "outputs": [],
      "source": [
        "print(type(datagroup))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD8JoHh_KEtJ"
      },
      "outputs": [],
      "source": [
        "sum_ = datagroup['secondary_labels'].sum()\n",
        "distri = datagroup['secondary_labels']/float(sum_)\n",
        "class_labels = list(datagroup.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_0jGCclKGf5"
      },
      "outputs": [],
      "source": [
        "y=np.array([35,25,25,15])\n",
        "plt.pie(distri)#autopct=\"%.2f%%\")#labels=class_labels)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpwrgkvIKLE5"
      },
      "source": [
        "### Visualization of Duration time(seconds) distribution of each species\n",
        "In this part, we visualize the duration of each species. Here, we used a violin plot to visualize the distribution of duration for each species, because there are 264 categories in total and one plot cannot put down all of them, so we divided the plot into 7 subplots so that the data can be viewed a little more clearly. The violin plot is a good representation of the density, frequency, median, and quantile of each array. As you can see from the graph, the distribution of categories in this dataset is not quite balanced, in line with our visualization of category distribution in the previous section, where some categories have a larger number of violins and some categories have a smaller number of violins and a smaller area. However, from the figure, the distribution of duration of each category is relatively concentrated in the same location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veAIuJQdKP35"
      },
      "outputs": [],
      "source": [
        "def getduration(i): \n",
        "    \"\"\"\n",
        "    display waveform of a given speech sample\n",
        "    :param sample_name: speech sample name\n",
        "    :param fs: sample frequency\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    file_prefix = \"/home/yangya/桌面/project/train_audio/\"\n",
        "    idx = i\n",
        "    sample = data.iloc[idx]\n",
        "    path = file_prefix + sample[\"filename\"]\n",
        "    samples, sr = librosa.load(path, sr=16000)\n",
        "    # samples = samples[6000:16000]\n",
        "    avg_amp =librosa.get_duration(y=samples,sr=sr)\n",
        "    return avg_amp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlEpRdxAKRqP"
      },
      "outputs": [],
      "source": [
        "getduration(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q5yMjHpKUGx"
      },
      "outputs": [],
      "source": [
        "data_avg1 = {}\n",
        "for i in range(len(class_labels)):\n",
        "    data_avg1[class_labels[i]] = []\n",
        "for i in range(len(data)):\n",
        "    temp = getduration(i)\n",
        "    label = data['primary_label'][i]\n",
        "    data_avg1[label].append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42aAz9v_KXhp"
      },
      "outputs": [],
      "source": [
        "df1_1=[]\n",
        "for i in range(len(class_labels)):\n",
        "    df1_1.append(data_avg1[class_labels[i]])\n",
        "print(len(class_labels))\n",
        "print(len(df1_1))\n",
        "df1_2=df1_1[:40]\n",
        "df1_3=df1_1[41:80]\n",
        "df1_4=df1_1[81:120]\n",
        "df1_5=df1_1[121:160]\n",
        "df1_6=df1_1[161:220]\n",
        "df1_7=df1_1[221:264]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo5pk4CTKZOB"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT_OPUvcKZ-J"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T0xPNSeKbia"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouSAPqYiKdOZ"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oLs5b7XKe2B"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i9McearKgVZ"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df1_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD8s3ie_NQLK"
      },
      "source": [
        "## Visualization of average mfcc distribution of each species\n",
        "In this part, we visualize the average mfcc of each species. The figures tell that the distribution of average mfcc is signicantly different. So we use the mfccs feature to do the classification using traditional classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8a4kttgNVVJ"
      },
      "outputs": [],
      "source": [
        "def get_mean_mfcc(i):\n",
        "    ile_prefix = \"/home/yangya/桌面/project/train_audio/\"\n",
        "    idx = i\n",
        "    sample = data.iloc[idx]\n",
        "    path = file_prefix + sample[\"filename\"]\n",
        "    y, sr = librosa.load(path)\n",
        "\n",
        "# 计算 MFCC 特征\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    return (mfccs.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsM5IjpUNXvx"
      },
      "outputs": [],
      "source": [
        "data_avg2 = {}\n",
        "for i in range(len(class_labels)):\n",
        "    data_avg2[class_labels[i]] = []\n",
        "for i in range(len(data)):\n",
        "    temp = get_mean_mfcc(i)\n",
        "    label = data['primary_label'][i]\n",
        "    data_avg2[label].append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY1V6jR4NZrH"
      },
      "outputs": [],
      "source": [
        "df2_1=[]\n",
        "for i in range(len(class_labels)):\n",
        "    df2_1.append(data_avg2[class_labels[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L658629oNa_f"
      },
      "outputs": [],
      "source": [
        "print(len(class_labels))\n",
        "print(len(df2_1))\n",
        "df2_2=df2_1[:40]\n",
        "df2_3=df2_1[41:80]\n",
        "df2_4=df2_1[81:120]\n",
        "df2_5=df2_1[121:160]\n",
        "df2_6=df2_1[161:220]\n",
        "df2_7=df2_1[221:264]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F7QtQVLNdWZ"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTOUsQ10Ne4o"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjFZw5cWNfpG"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p52HdxYlNiyB"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvMWwOxLNkXj"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VmEglqNNmEw"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df2_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bkXze2hPPVn"
      },
      "source": [
        "### Visualization of average Amplitude distribution of each species\n",
        "In this part, we visualize the average amplitude distribution of each species. The figures tell that the average amplitude vary little. In the amplitude distribution chart, the median for all species is around 0.03,0.02."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL3uu3vSPRrH"
      },
      "outputs": [],
      "source": [
        "def dcalavgamplitude(i): \n",
        "    \"\"\"\n",
        "    display waveform of a given speech sample\n",
        "    :param sample_name: speech sample name\n",
        "    :param fs: sample frequency\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    file_prefix = \"/home/yangya/桌面/project/train_audio/\"\n",
        "    idx = i\n",
        "    sample = data.iloc[idx]\n",
        "    path = file_prefix + sample[\"filename\"]\n",
        "    samples, sr = librosa.load(path, sr=16000)\n",
        "    # samples = samples[6000:16000]\n",
        "    avg_amp =abs(samples).mean()\n",
        "    return avg_amp\n",
        "    # \n",
        "    # print(len(samples), sr)\n",
        "    # time = np.arange(0, len(samples)) * (1.0 / sr)\n",
        "    # plt.plot(time, samples)\n",
        "    # plt.title(\"time v.s. amplitude\")\n",
        "    # plt.xlabel(\"time(s)\")\n",
        "    # plt.ylabel(\"Amplitude\")\n",
        "    # # plt.savefig(\"your dir\\语音信号时域波形图\", dpi=600)\n",
        "    # plt.show()\n",
        "    # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2RAkp6XPTpM"
      },
      "outputs": [],
      "source": [
        "data_avg = {}\n",
        "for i in range(len(class_labels)):\n",
        "    data_avg[class_labels[i]] = []\n",
        "for i in range(len(data)):\n",
        "    temp = dcalavgamplitude(i)\n",
        "    label = data['primary_label'][i]\n",
        "    data_avg[label].append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7tXq51VPVN5"
      },
      "outputs": [],
      "source": [
        "#data_1 = {'labels':all_class_labels,'amplitude':mean_avg}\n",
        "\n",
        "#df_1 = pd.DataFrame(data_avg)\n",
        "df_1=[]\n",
        "for i in range(len(class_labels)):\n",
        "    df_1.append(data_avg[class_labels[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Zlua8KRPWms"
      },
      "outputs": [],
      "source": [
        "print(len(class_labels))\n",
        "print(len(df_1))\n",
        "df_2=df_1[:40]\n",
        "df_3=df_1[41:80]\n",
        "df_4=df_1[81:120]\n",
        "df_5=df_1[121:160]\n",
        "df_6=df_1[161:220]\n",
        "df_7=df_1[221:264]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k44if_r5PX2W"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSyT6hkiPZW8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNtuMuQVPbDs"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1U1DyYYPcbI"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pZTOsrzPdxO"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAOOPudhPfJk"
      },
      "outputs": [],
      "source": [
        "#m = [2,3,4,5,6,7,8,9]\n",
        "_, ax = plt.subplots(figsize=(100,10))\n",
        "#df_1 = pd.DataFrame(c)\n",
        "sns.violinplot(ax=ax, data=df_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYtMAkQwOuYC"
      },
      "source": [
        "# **Traditional Classifier**\n",
        "## Outline\n",
        "### 1. Features: MFCCs\n",
        "- Data Preprocessing\n",
        "- Classifiers\n",
        "     - **KMeans Clustering**\n",
        "      - Naive Bayes Multinomial BOW\n",
        "      - Naive Bayes Multinomial TF-IDF\n",
        "      - Suport Vector Machine(SVM) TF-IDF using linear kernel\n",
        "      - Support Vector Machine (SVM) TF-IDF using linear kernel with cross-validation\n",
        "      - Support Vector Machine (SVM) TF-IDF using RBF kernel\n",
        "      - Support Vector Machine (SVM) TF-IDF using RBF kernel with cross-validation\n",
        "      - Gradient Boosting TF-IDF with cross-validation\n",
        "      - Random Forest TF-IDF with cross-validation\n",
        "      - Logistic Regression TF-IDF\n",
        "      - Logistic Regression TF-IDF with cross-validation\n",
        "\n",
        "      - Use ***PCA*** to reduce the feature dimensionality\n",
        "        - SVM (kernel='rbf')  (features dimensionality reduced by PCA)\n",
        "     - **MeanShitft Clustering**\n",
        "      - SVM(kernel='rbf')\n",
        "      - Use **PCA** to reduce the feature dimensionality \n",
        "        - SVM (kernel='rbf')  (features dimensionality reduced by PCA)\n",
        "     - **GMM Clustering**\n",
        "      - SVM(kernel='rbf')\n",
        "      - Use **PCA** to reduce the feature dimensionality \n",
        "        - SVM (kernel='rbf')  (features dimensionality reduced by PCA)\n",
        "     - **Spectral Clustering**\n",
        "      - SVM(kernel='rbf')\n",
        "      - Use **PCA** to reduce the feature dimensionality \n",
        "        - SVM (kernel='rbf')  (features dimensionality reduced by PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kb12sPdSkx3"
      },
      "source": [
        "Firstly, we use panda.read_csv to read the whole data and it returns data with type of Dataframe. And then ,we define a function to convert the audio file into mfccs features by using functions implemented by librosa. After this, we use clustering methods to find the most distinct mfccs features and map the mfccs features of each audio to the index of clustering centers. We use the processed data to do the classification.\n",
        "\n",
        "Here are the analysis of our traditional machine learning classifier.\n",
        "In the beginning, we split the whole data into traindata and test data, and use the traindata to train the classifer with cross-validation, then we use the testdata to evaluate the classifier. We create the bag of audios and convert the auido file into the index of audio file in the bag. We get two representations of the audio file, thanks to feature_extraction.text.TfidfTransformer, which are BOW and TF-IDF. BoW just consider the frequency of a 'word' without the importance of a 'word',while the TF-IDF considers both the frequency and importance. We firstly use Naive Bayes Multinomial classfier to classify the species with BOW and TF-IDF respectively, and the features TF-IDF works better. So in the next experiments, we use TF-IDF to do the classification. Though we know when TF-IDF works better with Naive Bayes Multinomial, it does not mean it will works better with other classifiers, we just simplify the problem. So that wo needn't to run so many classifer which just cost my time with no other techniques. \n",
        "In the next experiments, we use SVM(kernel = linear), SVM(kernel=rbf), Gradient Boosting , Random Forest and LR with cross-validation to do the classification. Among all the classifer, SVM(kernel=rbf) works best, so we use the classifer to do the classification after we use PCA to reduce the dimensionality of TF-IDF features. We use PCA to reduce the dimensionality after each clustering method. In order to find the if the performance affected by clustering method, we try different clustering method, which includs KMeans, Meanshift, GMM,and Spectral Clustering method. We find that the clustering method has little influence on the performance.BTW，SVM(kernel=rbf）works best when using KMeans and TF-IDF , so when using different clustering method, we use SVM(kernel = rbf) to evaluate the peroformance. However, the performance is ver bad. In order to improve the performance, we consider using DNN models. We will analysis it in the next part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLS-gZ6JSC1o"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBRZMUvqSCT1"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/home/yangya/桌面/project/train_metadata.csv/train_metadata.csv\",engine='python')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyNt-nxsHRvA"
      },
      "source": [
        "Split the data into traindata and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU_qi12_HQ0V"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j3mfyLNIB1v"
      },
      "source": [
        "A test to convert the ogg file into mfccs features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arib11zyIGKF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 加载一个音频文件\n",
        "file_prefix = \"/home/yangya/桌面/project/train_audio/\"\n",
        "idx = 5\n",
        "sample = data.iloc[idx]\n",
        "path = file_prefix + sample[\"filename\"]\n",
        "y, sr = librosa.load(path)\n",
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHZu-GQeIJBP"
      },
      "outputs": [],
      "source": [
        "print(mfccs)\n",
        "print(np.array(mfccs).T.shape)#将计算得到的mfccs保存成转置形式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ebPtbBcIQTV"
      },
      "source": [
        "Data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sefZu15rSCY7"
      },
      "outputs": [],
      "source": [
        "#convert ogg file into mfccs\n",
        "def convert_to_mfccs(data):\n",
        "    mfccs_res = []\n",
        "    file_prefix = \"/home/yangya/桌面/project/train_audio/\"\n",
        "    for i in range(len(data)):\n",
        "        idx =i \n",
        "        sample = data.iloc[idx]\n",
        "        path = file_prefix + sample[\"filename\"]\n",
        "        y, sr = librosa.load(path)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        mfccs_res.append(mfccs.T)\n",
        "    return mfccs_res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3JD8mBPSPHo"
      },
      "outputs": [],
      "source": [
        "#get the mfccs feature \n",
        "train_mfccs = convert_to_mfccs(train_df)\n",
        "test_mfccs= convert_to_mfccs(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGQ5fQ9QSQ9l"
      },
      "outputs": [],
      "source": [
        "# compute delta MFCCs\n",
        "def compute_delta_mfccs(mfccs):\n",
        "    dmfccs = []\n",
        "    for m in mfccs:\n",
        "        tmp = m[1:] - m[0:-1]\n",
        "        dm = np.hstack((m[0:-1], tmp))\n",
        "        dmfccs.append(dm)\n",
        "    return dmfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVeNE6H8STQi"
      },
      "outputs": [],
      "source": [
        "#get the compute delta mfccs\n",
        "train_dmfccs = compute_delta_mfccs(train_mfccs)\n",
        "test_dmfccs = compute_delta_mfccs(test_mfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D13-Zp9FSVbt"
      },
      "outputs": [],
      "source": [
        "all_dmfccs = np.vstack(train_dmfccs)\n",
        "print(all_dmfccs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B2Ky7vm_KYV"
      },
      "outputs": [],
      "source": [
        "tagnames = data[\"primary_label\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv53U0jy_LH6"
      },
      "outputs": [],
      "source": [
        "# convert list of tags into binary class labels\n",
        "def tags2class(tags, tagnames):\n",
        "    b = np.zeros(shape=(len(tags), len(tagnames)))\n",
        "    for i,t in enumerate(tags):\n",
        "        for j,n in enumerate(tagnames):\n",
        "            if n in t:\n",
        "                b[i,j] = 1\n",
        "    return b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B_3Jv3d_LMp"
      },
      "outputs": [],
      "source": [
        "# train_classes[i,j] = absence/presence of the j-th tag in the i-th sound\n",
        "train_classes_ = tags2class(train_df['primary_label'], tagnames)\n",
        "test_classes_ = tags2class(test_df['primary_label'], tagnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJrjqE46_OPV"
      },
      "outputs": [],
      "source": [
        "train_classes = []\n",
        "for i in range(len(train_classes_)):\n",
        "    train_classes.append(np.argmax(train_classes_[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO5ymqr1_QHp"
      },
      "outputs": [],
      "source": [
        "test_classes = []\n",
        "for j in range(len(test_classes_)):\n",
        "    test_classes.append(np.argmax(test_classes_[j]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoN3-VmD_SBK"
      },
      "outputs": [],
      "source": [
        "print(np.array(train_classes).shape)\n",
        "print(np.array(test_classes).shape)\n",
        "#print(test_classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiwbHbKDSdYW"
      },
      "source": [
        "## KMeans Clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdUthkwLSc2s"
      },
      "outputs": [],
      "source": [
        "# run k-means to build codebook\n",
        "km = cluster.KMeans(n_clusters=100, random_state=4487)\n",
        "km.fit(all_dmfccs[0::100])  # subsample by 10 to make it faster\n",
        "km.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4A2Bs2n-8ZH"
      },
      "outputs": [],
      "source": [
        "def bow_transform(model, mfccs):\n",
        "    numwords = model.cluster_centers_.shape[0]\n",
        "    bows = np.zeros((len(mfccs), numwords))\n",
        "    for i in range(len(mfccs)):\n",
        "        w = model.predict(mfccs[i])\n",
        "        bw = np.bincount(w, minlength=numwords)\n",
        "        bows[i,:] = bw\n",
        "    return bows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6eJkixYIY3f"
      },
      "source": [
        "BoW representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXDWEj_W--Ni"
      },
      "outputs": [],
      "source": [
        "train_bow = bow_transform(km, train_dmfccs)\n",
        "test_bow = bow_transform(km,test_dmfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynaYMqul_AiS"
      },
      "outputs": [],
      "source": [
        "print(train_bow.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDGd5k57IcAN"
      },
      "source": [
        "TF-IDF representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgmptD9c_CHK"
      },
      "outputs": [],
      "source": [
        "# convert to TF\n",
        "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
        "train_Xtf = tf_trans.fit_transform(train_bow)\n",
        "test_Xtf  = tf_trans.transform(test_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfGNdmVQI7gt"
      },
      "source": [
        "Classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6C1uq5r_Zfq"
      },
      "source": [
        "## Naive Bayes Multinomial BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7LUvqXk_YZ7"
      },
      "outputs": [],
      "source": [
        "# Train Multinomial NB model\n",
        "def trainMModel(a):\n",
        "    mmodel = naive_bayes.MultinomialNB(alpha = a)\n",
        "    mmodel.fit(train_bow,train_classes)\n",
        "    \n",
        "    testY = test_classes\n",
        "    predY = mmodel.predict(test_bow)\n",
        "    acc = metrics.accuracy_score(testY, predY)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "# Grid Search to find the best performance parameter setting (alpha)\n",
        "def GridSearchMModel(start_a, num_a):\n",
        "    best_a = start_a\n",
        "    best_acc = 0.0\n",
        "    for i in range(0, num_a):\n",
        "        a = start_a + (i / (num_a - 1))\n",
        "        tmp_acc = trainMModel(a)\n",
        "        if tmp_acc > best_acc:\n",
        "            best_a = a\n",
        "            best_acc = tmp_acc\n",
        "    return best_a, best_acc\n",
        "\n",
        "best_a, best_acc = GridSearchMModel(0.0, 10001)\n",
        "print(\"Parameter setting with best performance: alpha = {}, accuracy = {}\".format(best_a, best_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoFse6BM_hGc"
      },
      "source": [
        "## Naive Bayes Multinomial TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y9bV8NX_gG6"
      },
      "outputs": [],
      "source": [
        "# Train Multinomial NB model\n",
        "def trainMModel(a):\n",
        "    mmodel = naive_bayes.MultinomialNB(alpha = a)\n",
        "    mmodel.fit(train_Xtf,train_classes)\n",
        "    \n",
        "    testY = test_classes\n",
        "    predY = mmodel.predict(test_Xtf)\n",
        "    acc = metrics.accuracy_score(testY, predY)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "# Grid Search to find the best performance parameter setting (alpha)\n",
        "def GridSearchMModel(start_a, num_a):\n",
        "    best_a = start_a\n",
        "    best_acc = 0.0\n",
        "    for i in range(0, num_a):\n",
        "        a = start_a + (i / (num_a - 1))\n",
        "        tmp_acc = trainMModel(a)\n",
        "        if tmp_acc > best_acc:\n",
        "            best_a = a\n",
        "            best_acc = tmp_acc\n",
        "    return best_a, best_acc\n",
        "\n",
        "best_a, best_acc = GridSearchMModel(0.0, 10001)\n",
        "print(\"Parameter setting with best performance: alpha = {}, accuracy = {}\".format(best_a, best_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IXpTeQS_oKp"
      },
      "source": [
        "TF-IDF is better than BOW, so in the next experiments, we use TF-IDF as features to classify species"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOnjMHQy_qsR"
      },
      "source": [
        "## Suport Vector Machine(SVM) using linear kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iDoZrMj_peS"
      },
      "outputs": [],
      "source": [
        "# Using SVM with linear kernel\n",
        "def trainSVM(c):\n",
        "    clf = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', svm.SVC(C = c, kernel = 'linear'))])\n",
        "    svm_clf = clf.fit(train_Xtf, train_classes)\n",
        "    svm_predY = svm_clf.predict(test_Xtf)\n",
        "    acc_svm = metrics.accuracy_score(test_classes, svm_predY)\n",
        "    \n",
        "    return acc_svm\n",
        "\n",
        "# Grid Search to find the C with best performance\n",
        "def GridSearchSVM(Cs):\n",
        "    best_c = Cs[0]\n",
        "    best_acc = 0.0\n",
        "    for i in range(len(Cs)):\n",
        "        tmp_acc = trainSVM(Cs[i])\n",
        "        if tmp_acc > best_acc:\n",
        "            best_c = Cs[i]\n",
        "            best_acc = tmp_acc\n",
        "    return best_c, best_acc\n",
        "\n",
        "Cs = np.logspace(-5, 5, 50)\n",
        "best_c, best_acc_svm = GridSearchSVM(Cs)\n",
        "print(\"Parameter setting with best performance: C = {}, accuracy = {}\".format(best_c, best_acc_svm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFdCtVp_wSs"
      },
      "source": [
        "## Support Vector Machine (SVM) using linear kernel with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcea-TcR_zdy"
      },
      "outputs": [],
      "source": [
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv = model_selection.GridSearchCV(svm.SVC(kernel = 'linear'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv.fit(train_Xtf , train_classes);\n",
        "\n",
        "print(\"best params:\", svmcv.best_params_)\n",
        "# predict from the model\n",
        "predY1 = svmcv.best_estimator_.predict(test_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc1 = metrics.accuracy_score(test_classes, predY1)\n",
        "print(\"test accuracy =\", acc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXntS4W_1wj"
      },
      "source": [
        "## Support Vector Machine (SVM) using RBF kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f_BRmOo_4Dy"
      },
      "outputs": [],
      "source": [
        "# Using SVM with linear kernel\n",
        "def trainSVMRBF(c):\n",
        "    clf = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', svm.SVC(C = c, kernel = 'rbf'))])\n",
        "    svm_clf = clf.fit(train_Xtf, train_classes)\n",
        "    svm_predY = svm_clf.predict(test_Xtf)\n",
        "    acc_svm = metrics.accuracy_score(test_classes, svm_predY)\n",
        "    \n",
        "    return acc_svm\n",
        "\n",
        "# Grid Search to find the C with best performance\n",
        "def GridSearchSVMRBF(Cs):\n",
        "    best_c = Cs[0]\n",
        "    best_acc = 0.0\n",
        "    for i in range(len(Cs)):\n",
        "        tmp_acc = trainSVMRBF(Cs[i])\n",
        "        if tmp_acc > best_acc:\n",
        "            best_c = Cs[i]\n",
        "            best_acc = tmp_acc\n",
        "    return best_c, best_acc\n",
        "\n",
        "Cs = np.logspace(-5, 5, 50)\n",
        "best_c3, best_acc_svm3 = GridSearchSVMRBF(Cs)\n",
        "print(\"Parameter setting with best performance: C = {}, accuracy = {}\".format(best_c3, best_acc_svm3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kOykuMC_5_N"
      },
      "source": [
        "## Support Vector Machine (SVM) using RBF kernel with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI0GwAnD_8sp"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(train_Xtf, train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(test_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s36JiMNX_-n5"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J6bZmrrAAhm"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "# Gradient Boosting\n",
        "\n",
        "paramsampler= {    \n",
        "    \"colsample_bytree\": stats.uniform(0.7, 0.3),  \n",
        "    \"gamma\":            stats.uniform(0, 0.5),    \n",
        "    \"max_depth\":        stats.randint(2, 6),      \n",
        "    \"subsample\":        stats.uniform(0.6, 0.4),  \n",
        "    \"learning_rate\":    stats.uniform(.001,1),    \n",
        "    \"n_estimators\":     stats.randint(10, 1000),\n",
        "}\n",
        "\n",
        "#X_train, X_test, y_train, y_test = model_selection.train_test_split(trainXtf, trainY, test_size = 0.2, random_state = 0)\n",
        "xclf = xgb.XGBClassifier(objective = \"multi:softmax\", random_state = 4487)\n",
        "xgbcv = model_selection.RandomizedSearchCV(xclf, param_distributions = paramsampler, random_state = 4487, n_iter = 200, cv = 5, verbose = 1, n_jobs = -1)\n",
        "xgbcv.fit(train_Xtf , train_classes)\n",
        "\n",
        "print(\"best params:\", xgbcv.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO1iC3iKACch"
      },
      "outputs": [],
      "source": [
        "xgb_predY = xgbcv.best_estimator_.predict(test_Xtf)\n",
        "acc_xgb = metrics.accuracy_score(test_classes, xgb_predY)\n",
        "print(\"Gradient Boosting classifier accuracy: {}\".format(acc_xgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4Fuu4AkAEIZ"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caaGFEBNAGBS"
      },
      "outputs": [],
      "source": [
        "paramsampler = {#'max_features': stats.uniform(0,1.0),\n",
        "                 'max_depth':         stats.randint(1,5),\n",
        "                 'min_samples_split': stats.uniform(0,0.5), \n",
        "                 'min_samples_leaf':  stats.uniform(0,0.5),\n",
        "               }\n",
        "\n",
        "rfrcv = model_selection.RandomizedSearchCV(\n",
        "                            ensemble.RandomForestClassifier(n_estimators = 100, random_state = 4487, n_jobs = -1),\n",
        "                            param_distributions = paramsampler, \n",
        "                            random_state = 4487, n_iter = 1000, cv = 5, \n",
        "                            verbose = 1, n_jobs = -1)\n",
        "\n",
        "rfrcv.fit(train_Xtf, train_classes);\n",
        "\n",
        "print(\"best params:\", rfrcv.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1wERXA7AHki"
      },
      "outputs": [],
      "source": [
        "rf_predY = rfrcv.best_estimator_.predict(test_Xtf)\n",
        "acc_rf = metrics.accuracy_score(test_classes, rf_predY)\n",
        "print(\"Random Forest classifier accuracy: {}\".format(acc_rf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC8-vfO0AJZ9"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAbkJIroALWK"
      },
      "outputs": [],
      "source": [
        "# Using LR\n",
        "def trainLR(c):\n",
        "    logreg = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', linear_model.LogisticRegression(n_jobs = -1, C = c, solver = 'saga'))])\n",
        "    lr_clf = logreg.fit(train_Xtf, train_classes)\n",
        "    lr_predY = lr_clf.predict(test_Xtf)\n",
        "    acc_lr = metrics.accuracy_score(test_classes, lr_predY)\n",
        "    \n",
        "    return acc_lr\n",
        "\n",
        "# Grid Search to find the C with best performance\n",
        "def GridSearchLR(Cs):\n",
        "    best_c = Cs[0]\n",
        "    best_acc = 0.0\n",
        "    for i in range(len(Cs)):\n",
        "        tmp_acc = trainLR(Cs[i])\n",
        "        if tmp_acc > best_acc:\n",
        "            best_c = Cs[i]\n",
        "            best_acc = tmp_acc\n",
        "    return best_c, best_acc\n",
        "\n",
        "Cs = logspace(-5, 5, 50)\n",
        "best_c_lr, best_acc_lr = GridSearchLR(Cs)\n",
        "print(\"Parameter setting with best performance: C = {}, accuracy = {}\".format(best_c_lr, best_acc_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e8J2zhANTa"
      },
      "source": [
        "## Logistic Regression with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CC-PtBxAP1j"
      },
      "outputs": [],
      "source": [
        "Cs = logspace(-5, 5, 50)\n",
        "\n",
        "# setup the cross-validation object\n",
        "lrcv_clf = pipeline.Pipeline([('vect', feature_extraction.text.CountVectorizer()), ('tfidf', feature_extraction.text.TfidfTransformer()), ('clf', linear_model.LogisticRegressionCV(n_jobs = -1, Cs = Cs, solver = 'saga', max_iter = 10000))])\n",
        "lrcv = lrcv_clf.fit(train_Xtf, train_classes)\n",
        "\n",
        "lr_predY = lrcv.predict(test_Xtf)\n",
        "acc_lrcv = metrics.accuracy_score(test_classes, lr_predY)\n",
        "\n",
        "print(\"Accuracy of LR model with cross-validation: \", acc_lrcv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwb2tgVCASbh"
      },
      "source": [
        "## Use PCA to reduce dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYK7guNqAVbR"
      },
      "outputs": [],
      "source": [
        "pca_model = decomposition.TruncatedSVD(n_components=90)\n",
        "pca_model.fit(train_Xtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gw5MC9iAW7l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je7s7YHaAYQa"
      },
      "outputs": [],
      "source": [
        "# This function plot the PCA curve\n",
        "def plot_exp_ratio(ratio, title):\n",
        "    explain_fig = plt.figure()\n",
        "    idx = np.where(ratio > 0.95)[0]#[0]\n",
        "    print(\"95% ratio when components are {}\".format(idx))\n",
        "    #plt.title(title)\n",
        "    #plt.plot(ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gl6aIe2AbtZ"
      },
      "outputs": [],
      "source": [
        "#print((np.cumsum(pca_model.explained_variance_ratio_)).shape)\n",
        "plot_exp_ratio(np.cumsum(pca_model.explained_variance_ratio_), \n",
        "               \"Explained Variance Ratio(PCA)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuqluv17AcZ5"
      },
      "source": [
        "## Covert the tran_Xtf and test_xtf to reduced dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH5A01S_Ae1x"
      },
      "outputs": [],
      "source": [
        "pca_500To285 = decomposition.TruncatedSVD(n_components=76)\n",
        "pca_500To285.fit(train_Xtf)\n",
        "\n",
        "train_Xtfpca = pca_500To285.transform(train_Xtf)\n",
        "test_Xtfpca = pca_500To285.transform(test_Xtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miOJgA9QAgTJ"
      },
      "outputs": [],
      "source": [
        "print(train_Xtfpca.shape)\n",
        "print(test_Xtfpca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukOWcqetAiHh"
      },
      "source": [
        "## Use SVM (kernel='rbf') to estimate [features dimensionality reduced by PCA]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Gw7yD3dAkQw"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(train_Xtfpca, train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(test_Xtfpca)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hHcvRlFAoLC"
      },
      "source": [
        "# Mean-Shift Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpHdFPpzArvp"
      },
      "outputs": [],
      "source": [
        "# run k-means to build codebook\n",
        "km = cluster.MeanShift(bandwidth=5, bin_seeding=True, n_jobs=-1)#cluster.KMeans(n_clusters=100, random_state=4487)\n",
        "km.fit(all_dmfccs[0::100])  # subsample by 10 to make it faster\n",
        "km.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CViyO9coAyeL"
      },
      "outputs": [],
      "source": [
        "def bow_transform(model, mfccs):\n",
        "    numwords = model.cluster_centers_.shape[0]\n",
        "    bows = np.zeros((len(mfccs), numwords))\n",
        "    for i in range(len(mfccs)):\n",
        "        w = model.predict(mfccs[i])\n",
        "        bw = np.bincount(w, minlength=numwords)\n",
        "        bows[i,:] = bw\n",
        "    return bows\n",
        "\n",
        "trainmeanshift_bow = bow_transform(km, train_dmfccs)\n",
        "testmeanshift_bow = bow_transform(km,test_dmfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbH3RbAQA1JZ"
      },
      "outputs": [],
      "source": [
        "# convert to TF\n",
        "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
        "trainmeanshift_Xtf = tf_trans.fit_transform(trainmeanshift_bow )\n",
        "testmeanshift_Xtf  = tf_trans.transform(testmeanshift_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhAVAaO9A1xB"
      },
      "source": [
        "## SVM(kernel='rbf') [features extracted by Meanshift method]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofhUXigeA6LL"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(trainmeanshift_Xtf , train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(testmeanshift_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyryQ78eA8QY"
      },
      "source": [
        "## Use PCA to reduce dimensionality and then use SVM（rbf kernel） to do the classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmj8wuRkA-5h"
      },
      "outputs": [],
      "source": [
        "pca_500To285 = decomposition.TruncatedSVD(n_components=50)\n",
        "pca_500To285.fit(train_Xtf)\n",
        "\n",
        "train_Xtfpca = pca_500To285.transform(train_Xtf)\n",
        "test_Xtfpca = pca_500To285.transform(test_Xtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaxbSVmJBBj4"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(trainmeanshift_Xtf , train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(testmeanshift_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWesFHUnBD59"
      },
      "source": [
        "# GMM Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFg4ycNDBHJh"
      },
      "outputs": [],
      "source": [
        "# run k-means to build codebook\n",
        "km = mixture.GaussianMixture(n_components=100, covariance_type='full', random_state=4487, n_init=10)\n",
        "km.fit(all_dmfccs[0::100])  # subsample by 10 to make it faster\n",
        "km.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVP3tBJGBIrx"
      },
      "outputs": [],
      "source": [
        "def bow_transform(model, mfccs):\n",
        "    numwords = model.cluster_centers_.shape[0]\n",
        "    bows = np.zeros((len(mfccs), numwords))\n",
        "    for i in range(len(mfccs)):\n",
        "        w = model.predict(mfccs[i])\n",
        "        bw = np.bincount(w, minlength=numwords)\n",
        "        bows[i,:] = bw\n",
        "    return bows\n",
        "\n",
        "trainGMM_bow = bow_transform(km, train_dmfccs)\n",
        "testGMM_bow = bow_transform(km,test_dmfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au4l1cPhBKQB"
      },
      "outputs": [],
      "source": [
        "# convert to TF\n",
        "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
        "trainGMM_Xtf = tf_trans.fit_transform(trainGMM_bow )\n",
        "testGMM_Xtf  = tf_trans.transform(testGMM_bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpAqWD6GBMMC"
      },
      "source": [
        "## SVM(kernel='rbf') [features extracted by GMM method]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZmVrcFaBPDe"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(trainGMM_Xtf , train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(testGMM_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nn4OT_EoBRQZ"
      },
      "source": [
        "## Use PCA to reduce dimensionality from features extracted by GMM clustering and then use SVM（rbf kernel） to do the classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3B8OmmOBTXh"
      },
      "outputs": [],
      "source": [
        "pca_500To285 = decomposition.TruncatedSVD(n_components=50)\n",
        "pca_500To285.fit(trainGMM_Xtf)\n",
        "\n",
        "train_Xtfpca = pca_500To285.transform(trainGMM_Xtf)\n",
        "test_Xtfpca = pca_500To285.transform(testGMM_Xtf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJYZvsN2BV9-"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(train_Xtfpca , train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(test_Xtfpca)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOtiFd8wBW_5"
      },
      "source": [
        "# Spectral Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXAtL5HPBZsW"
      },
      "outputs": [],
      "source": [
        "# run k-means to build codebook\n",
        "km = luster.SpectralClustering(n_clusters=4100, affinity='rbf', gamma=1.0, assign_labels='discretize', n_jobs=-1)\n",
        "km.fit(all_dmfccs[0::100])  # subsample by 10 to make it faster\n",
        "km.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rvXvVuABbG5"
      },
      "outputs": [],
      "source": [
        "def bow_transform(model, mfccs):\n",
        "    numwords = model.cluster_centers_.shape[0]\n",
        "    bows = np.zeros((len(mfccs), numwords))\n",
        "    for i in range(len(mfccs)):\n",
        "        w = model.predict(mfccs[i])\n",
        "        bw = np.bincount(w, minlength=numwords)\n",
        "        bows[i,:] = bw\n",
        "    return bows\n",
        "\n",
        "trainS_bow = bow_transform(km, train_dmfccs)\n",
        "testS_bow = bow_transform(km,test_dmfccs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31EGdz0NBdC9"
      },
      "outputs": [],
      "source": [
        "pca_500To285 = decomposition.TruncatedSVD(n_components=50)\n",
        "pca_500To285.fit(trainGMM_Xtf)\n",
        "\n",
        "train_Xtfpca = pca_500To285.transform(trainGMM_Xtf)\n",
        "test_Xtfpca = pca_500To285.transform(testGMM_Xtf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSlkvqlmBfiZ"
      },
      "source": [
        "## Use PCA to reduce dimensionality of features extracted by Spectral clustering method and then use SVM（rbf） to do the classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3NILTyMBhn6"
      },
      "outputs": [],
      "source": [
        "# setup the list of parameters to try\n",
        "paramgrid = {'C': np.logspace(-5, 5, 50)}\n",
        "\n",
        "print(paramgrid)\n",
        "\n",
        "# setup the cross-validation object\n",
        "# pass the SVM object w/ rbf kernel, parameter grid, and number of CV folds\n",
        "svmcv2 = model_selection.GridSearchCV(svm.SVC(kernel = 'rbf'), paramgrid, cv=5, n_jobs=-1, verbose=True)\n",
        "\n",
        "# run cross-validation (train for each split)\n",
        "svmcv2.fit(trainS_Xtf , train_classes)\n",
        "\n",
        "print(\"best params:\", svmcv2.best_params_)\n",
        "\n",
        "# predict from the model\n",
        "predY2 = svmcv2.best_estimator_.predict(testS_Xtf)\n",
        "\n",
        "# calculate accuracy\n",
        "acc2 = metrics.accuracy_score(test_classes, predY2)\n",
        "print(\"test accuracy =\", acc2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7ZzKAPuVtvv"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vwXSJ3M-JWTy",
        "f8yHGW9AJijh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
