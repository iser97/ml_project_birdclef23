{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install efficientnet_pytorch\n",
    "! pip install prefetch-generator\n",
    "! pip install torchaudio_augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plathzheng/miniconda3/envs/tutorial/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-18 17:43:23.687438: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-18 17:43:23.721228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-18 17:43:24.213054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None # avoids assignment warning\n",
    "import numpy as np\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # enable progress bars in pandas operations\n",
    "import gc\n",
    "\n",
    "import librosa\n",
    "import sklearn\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Import for visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as lid\n",
    "import IPython.display as ipd\n",
    "\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import ASTFeatureExtractor\n",
    "from transformers import ASTPreTrainedModel, ASTModel, AutoConfig, ASTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CFG:\n",
    "    # Debugging\n",
    "    debug = False\n",
    "    \n",
    "    # Plot training history\n",
    "    training_plot = True\n",
    "    \n",
    "    # Weights and Biases logging\n",
    "    wandb = True\n",
    "    competition   = 'birdclef-2023' \n",
    "    _wandb_kernel = 'awsaf49'\n",
    "    \n",
    "    # Experiment name and comment\n",
    "    exp_name = 'baseline-v2'\n",
    "    comment = 'EfficientNetB0|FSR|t=10s|128x384|up_thr=50|cv_filter'\n",
    "    \n",
    "    # Notebook link\n",
    "    notebook_link = 'https://www.kaggle.com/awsaf49/birdclef23-effnet-fsr-cutmixup-train/edit'\n",
    "    \n",
    "    # Verbosity level\n",
    "    verbose = 0\n",
    "    \n",
    "    # Device and random seed\n",
    "    device = 'TPU-VM'\n",
    "    seed = 42\n",
    "    \n",
    "    # Input image size and batch size\n",
    "    img_size = [128, 384]\n",
    "    batch_size = 32\n",
    "    upsample_thr = 50 # min sample of each class (upsample)\n",
    "    cv_filter = True # always keeps low sample data in train\n",
    "    \n",
    "    # Inference batch size, test time augmentation, and drop remainder\n",
    "    infer_bs = 2\n",
    "    tta = 1\n",
    "    drop_remainder = True\n",
    "    \n",
    "    # Number of epochs, model name, and number of folds\n",
    "    epochs = 25\n",
    "    model_name = 'EfficientNetB0'\n",
    "    fsr = True # reduce stride of stem block\n",
    "    num_fold = 5\n",
    "    \n",
    "    # Selected folds for training and evaluation\n",
    "    selected_folds = [0]\n",
    "\n",
    "    # Pretraining, neck features, and final activation function\n",
    "    pretrain = 'imagenet'\n",
    "    neck_features = 0\n",
    "    final_act = 'softmax'\n",
    "    \n",
    "    # Learning rate, optimizer, and scheduler\n",
    "    lr = 1e-3\n",
    "    scheduler = 'cos'\n",
    "    optimizer = 'Adam' # AdamW, Adam\n",
    "    \n",
    "    # Loss function and label smoothing\n",
    "    loss = 'BCE' # BCE, CCE\n",
    "    \n",
    "    label_smoothing = 0.05 # label smoothing\n",
    "    \n",
    "    # Audio duration, sample rate, and length\n",
    "    duration = 10 # second\n",
    "    sample_rate = 32000\n",
    "    target_rate = 8000\n",
    "    audio_len = duration*sample_rate\n",
    "    \n",
    "    # STFT parameters\n",
    "    nfft = 2048\n",
    "    window = 2048\n",
    "    hop_length = audio_len // (img_size[1] - 1)\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    normalize = True\n",
    "    \n",
    "    # Data augmentation parameters\n",
    "    augment=True\n",
    "    \n",
    "    # Spec augment\n",
    "    spec_augment_prob = 0.80\n",
    "    \n",
    "    mixup_prob = 0.65\n",
    "    mixup_alpha = 0.5\n",
    "    \n",
    "    cutmix_prob = 0.0\n",
    "    cutmix_alpha = 0.5\n",
    "    \n",
    "    mask_prob = 0.65\n",
    "    freq_mask = 20\n",
    "    time_mask = 30\n",
    "\n",
    "\n",
    "    # Audio Augmentation Settings\n",
    "    audio_augment_prob = 0.5\n",
    "    \n",
    "    timeshift_prob = 0.0\n",
    "    \n",
    "    gn_prob = 0.35\n",
    "\n",
    "    # Data Preprocessing Settings\n",
    "    base_path = '/kaggle/input/birdclef-2023'  # for server: base_path = '/data/zjh_data/program/ml_project_birdclef23/birdclef-2023'\n",
    "    if not os.path.exists(base_path):\n",
    "        base_path = '/data/zjh_data/program/ml_project_birdclef23/birdclef-2023'\n",
    "    class_names = sorted(os.listdir('{}/train_audio'.format(base_path)))\n",
    "    num_classes = len(class_names)\n",
    "    class_labels = list(range(num_classes))\n",
    "    label2name = dict(zip(class_labels, class_names))\n",
    "    name2label = {v:k for k,v in label2name.items()}\n",
    "\n",
    "    # Training Settings\n",
    "    target_col = ['target']\n",
    "    tab_cols = ['filename']\n",
    "    monitor = 'auc'\n",
    "    \n",
    "    ### add by plathzheng\n",
    "    unilm_model_path = './pretrained_models/unilm/BEATs_iter3_plus_AS2M.pt'\n",
    "    use_apex = True\n",
    "    time_length = 10  # beats模型中，训练时，截取的音频片段时长\n",
    "    ast_fix_layer = 9 # the parameters in layer<ast_fix_layer would be fixed, choosen from [0, 5], if ast_fix_layer>5 all param woudl be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(CFG.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/birdclef-2023/test_soundscapes/s...</td>\n",
       "      <td>soundscape_29201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath          filename\n",
       "0  /kaggle/input/birdclef-2023/test_soundscapes/s...  soundscape_29201"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths = glob('/kaggle/input/birdclef-2023/test_soundscapes/*ogg')\n",
    "test_df = pd.DataFrame(test_paths, columns=['filepath'])\n",
    "test_df['filename'] = test_df.filepath.map(lambda x: x.split('/')[-1].replace('.ogg',''))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ast_feature_extractor = ASTFeatureExtractor()\n",
    "\n",
    "class ASTDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.paths = df.filepath.values\n",
    "        \n",
    "        # extract_ast_features(df)  # wheter to extract data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        raw, sr = librosa.load(path, sr=CFG.sample_rate, mono=True)\n",
    "        raw = librosa.resample(raw, orig_sr=CFG.sample_rate, target_sr=16000)  # ast can only process the audio with sr=16000\n",
    "        inputs = ast_feature_extractor(raw, sampling_rate=16000, return_tesnors='pt')\n",
    "        inputs = inputs['input_values'][0]\n",
    "        return path, inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "dataset_eval = ASTDataset(test_df)\n",
    "CFG.debug=True\n",
    "loader = DataLoader(dataset_eval, batch_size=CFG.batch_size, shuffle=False, drop_last=False, num_workers=0 if CFG.debug else 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, config, output_dim):\n",
    "        super().__init__()\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dense = nn.Linear(config.hidden_size, output_dim)\n",
    "    \n",
    "    def forward(self, hidden_state):\n",
    "        hidden_state = self.layernorm(hidden_state)\n",
    "        hidden_state = self.dense(hidden_state)\n",
    "        return hidden_state\n",
    "    \n",
    "class ASTagModel(ASTPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.audio_spectrogram_transformer = ASTModel(config)\n",
    "\n",
    "        for pname, p in self.named_parameters():\n",
    "            if pname.find('layer.') >= 0:\n",
    "                layer = int(pname.split('.')[3])\n",
    "                if layer<=CFG.ast_fix_layer:\n",
    "                    p.requires_grad = False\n",
    "            else:\n",
    "                p.requires_grad = False\n",
    "            \n",
    "        self.linear = DenseLayer(config, CFG.num_classes)\n",
    "        self.n_class = CFG.num_classes\n",
    "    \n",
    "    def forward(self, input_values):\n",
    "        outputs = self.audio_spectrogram_transformer(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pool_output = torch.mean(hidden_states, dim=1)\n",
    "        # pool_output = outputs.pooler_output\n",
    "        logits = self.linear(pool_output)\n",
    "        return nn.Sigmoid()(logits) if CFG.loss=='BCE' else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ASTConfig() \n",
    "model = ASTagModel(config=config)\n",
    "\n",
    "ckpt = torch.load('experiments/ast/trial_0/ast.pth', map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:47<00:00, 107.42s/it]\n"
     ]
    }
   ],
   "source": [
    "def file_load_chunk(audio_path, duration=5, sr=32000):\n",
    "    raw, sr = librosa.load(audio_path, sr=CFG.sample_rate, mono=True)\n",
    "    raw = librosa.resample(raw, orig_sr=CFG.sample_rate, target_sr=16000)  # ast can only process the audio with sr=16000\n",
    "    frame_length = int(duration*16000)\n",
    "    frame_step = int(duration*16000)\n",
    "    chunks = librosa.util.frame(raw, frame_length=frame_length, hop_length=frame_step, axis=0)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "model.eval()\n",
    "pred_stack = torch.randn(size=(1, CFG.num_classes)).to(device)\n",
    "ids = []\n",
    "for filepath in tqdm(test_df.filepath.tolist()):\n",
    "    filename = filepath.split('/')[-1].replace('.ogg','')\n",
    "    chunks = file_load_chunk(filepath)\n",
    "    \n",
    "    inputs = ast_feature_extractor(chunks.tolist(), sampling_rate=16000, return_tesnors='pt')\n",
    "    inputs = inputs['input_values']\n",
    "    inputs = np.stack(inputs)\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    if len(inputs.shape)==2:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        prob = model(inputs)\n",
    "    pred_stack = torch.cat([pred_stack, prob], dim=0)\n",
    "    ids += [f'{filename}_{(frame_id+1)*5}' for frame_id in range(len(chunks))]\n",
    "pred_stack = pred_stack[1:]\n",
    "preds = pred_stack.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>abethr1</th>\n",
       "      <th>abhori1</th>\n",
       "      <th>abythr1</th>\n",
       "      <th>afbfly1</th>\n",
       "      <th>afdfly1</th>\n",
       "      <th>afecuc1</th>\n",
       "      <th>affeag1</th>\n",
       "      <th>afgfly1</th>\n",
       "      <th>afghor1</th>\n",
       "      <th>...</th>\n",
       "      <th>yebsto1</th>\n",
       "      <th>yeccan1</th>\n",
       "      <th>yefcan</th>\n",
       "      <th>yelbis1</th>\n",
       "      <th>yenspu1</th>\n",
       "      <th>yertin1</th>\n",
       "      <th>yesbar1</th>\n",
       "      <th>yespet1</th>\n",
       "      <th>yetgre1</th>\n",
       "      <th>yewgre1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundscape_29201_5</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.060659</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.023877</td>\n",
       "      <td>0.029688</td>\n",
       "      <td>0.060907</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>0.137708</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.016115</td>\n",
       "      <td>0.004742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soundscape_29201_10</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.061043</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.023880</td>\n",
       "      <td>0.030620</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.013473</td>\n",
       "      <td>0.144527</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soundscape_29201_15</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>0.061248</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.030652</td>\n",
       "      <td>0.062929</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014275</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>0.143368</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.015586</td>\n",
       "      <td>0.004561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soundscape_29201_20</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.008335</td>\n",
       "      <td>0.060825</td>\n",
       "      <td>0.004328</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.062363</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014226</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.023714</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>0.013452</td>\n",
       "      <td>0.143186</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.015755</td>\n",
       "      <td>0.004628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soundscape_29201_25</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.060764</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.023844</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.063132</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.023841</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.143059</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.015701</td>\n",
       "      <td>0.004613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                row_id   abethr1   abhori1   abythr1   afbfly1   afdfly1  \\\n",
       "0   soundscape_29201_5  0.001188  0.008082  0.060659  0.004431  0.023877   \n",
       "1  soundscape_29201_10  0.001139  0.008449  0.061043  0.004339  0.023880   \n",
       "2  soundscape_29201_15  0.001139  0.008284  0.061248  0.004292  0.023421   \n",
       "3  soundscape_29201_20  0.001130  0.008335  0.060825  0.004328  0.023678   \n",
       "4  soundscape_29201_25  0.001148  0.008453  0.060764  0.004335  0.023844   \n",
       "\n",
       "    afecuc1   affeag1   afgfly1   afghor1  ...   yebsto1   yeccan1    yefcan  \\\n",
       "0  0.029688  0.060907  0.000638  0.000295  ...  0.013793  0.002948  0.023256   \n",
       "1  0.030620  0.063125  0.000606  0.000286  ...  0.014135  0.002871  0.023800   \n",
       "2  0.030652  0.062929  0.000604  0.000284  ...  0.014275  0.002887  0.023936   \n",
       "3  0.030282  0.062363  0.000611  0.000289  ...  0.014226  0.002891  0.023714   \n",
       "4  0.030457  0.063132  0.000617  0.000291  ...  0.014084  0.002899  0.023841   \n",
       "\n",
       "    yelbis1   yenspu1   yertin1   yesbar1   yespet1   yetgre1   yewgre1  \n",
       "0  0.004431  0.003432  0.013142  0.137708  0.002249  0.016115  0.004742  \n",
       "1  0.004516  0.003359  0.013473  0.144527  0.002333  0.015734  0.004600  \n",
       "2  0.004503  0.003334  0.013365  0.143368  0.002350  0.015586  0.004561  \n",
       "3  0.004505  0.003416  0.013452  0.143186  0.002365  0.015755  0.004628  \n",
       "4  0.004527  0.003400  0.013581  0.143059  0.002378  0.015701  0.004613  \n",
       "\n",
       "[5 rows x 265 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit prediction\n",
    "pred_df = pd.DataFrame(ids, columns=['row_id'])\n",
    "pred_df.loc[:, CFG.class_names] = preds\n",
    "pred_df.to_csv('submission.csv',index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna_utils.modules import *\n",
    "from optuna_utils.efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class Efficient(nn.Module):\n",
    "    def __init__(self, dataset='mtat'):\n",
    "        super(Efficient, self).__init__()\n",
    "        \n",
    "        self.model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        feature = self.model._fc.in_features\n",
    "        self.model._fc = nn.Linear(in_features=feature, out_features=CFG.num_classes, bias=True)\n",
    "\n",
    "        self.spec_bn = nn.BatchNorm2d(3)\n",
    "\n",
    "        # Pons front-end\n",
    "        m1 = Conv_V(3, 204, (int(0.7*96), 7))\n",
    "        m2 = Conv_V(3, 204, (int(0.4*96), 7))\n",
    "        m3 = Conv_H(3, 51, 129)\n",
    "        m4 = Conv_H(3, 51, 65)\n",
    "        m5 = Conv_H(3, 51, 33)\n",
    "        self.layers = nn.ModuleList([m1, m2, m3, m4, m5])\n",
    "\n",
    "        # Pons back-end\n",
    "        backend_channel= 512 if dataset=='msd' else 64\n",
    "        self.layer1 = Conv_1d(561, backend_channel, 7, 1, 1)\n",
    "        self.layer2 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "        self.layer3 = Conv_1d(backend_channel, backend_channel, 7, 1, 1)\n",
    "\n",
    "        # Dense\n",
    "        dense_channel = 500 if dataset=='msd' else 200\n",
    "        self.dense1 = nn.Linear((561+(backend_channel*3))*2, dense_channel)\n",
    "        self.bn = nn.BatchNorm1d(dense_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(dense_channel, CFG.num_classes)\n",
    "    \n",
    "    def forward(self, audio):\n",
    "        res = self.model(audio)\n",
    "        res = nn.Sigmoid()(res) if CFG.loss=='BCE' else res\n",
    "        return res\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "# model = Efficient()\n",
    "# ckpt = torch.load('experiments/efficient/trial_14/efficient.pth', map_location=device)\n",
    "# model.load_state_dict(ckpt)\n",
    "# model = model.to(device)\n",
    "model = torch.load('experiments/efficient_save/trial_0/efficient.pth', map_location='cpu')\n",
    "model = model.to(device)\n",
    "torch.save(model.state_dict(), 'efficient.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "def file_load_chunk(audio_path, duration=5, sr=32000):\n",
    "    raw, sr = librosa.load(audio_path, sr=CFG.sample_rate, mono=True)\n",
    "    raw = librosa.resample(raw, orig_sr=CFG.sample_rate, target_sr=16000)  # ast can only process the audio with sr=16000\n",
    "    frame_length = int(duration*16000)\n",
    "    frame_step = int(duration*16000)\n",
    "    chunks = librosa.util.frame(raw, frame_length=frame_length, hop_length=frame_step, axis=0)\n",
    "    chunks = [torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate, \n",
    "            n_fft=CFG.nfft, \n",
    "            win_length=CFG.window, \n",
    "            hop_length=CFG.hop_length, \n",
    "            f_min=CFG.fmin, \n",
    "            f_max=CFG.fmax, \n",
    "            n_mels=CFG.img_size[0])(torch.Tensor(wav)).unsqueeze(0).repeat(3, 1, 1) for wav in chunks.tolist()]\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "model.eval()\n",
    "pred_stack = torch.randn(size=(1, CFG.num_classes)).to(device)\n",
    "ids = []\n",
    "for filepath in tqdm(test_df.filepath.tolist()):\n",
    "    filename = filepath.split('/')[-1].replace('.ogg','')\n",
    "    chunks = file_load_chunk(filepath)\n",
    "    inputs = torch.stack(chunks)\n",
    "    if len(inputs.shape)==2:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        prob = model(inputs)\n",
    "    pred_stack = torch.cat([pred_stack, prob], dim=0)\n",
    "    ids += [f'{filename}_{(frame_id+1)*5}' for frame_id in range(len(inputs))]\n",
    "pred_stack = pred_stack[1:]\n",
    "preds = pred_stack.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna_utils.utils import *\n",
    "\n",
    "def eval(model, loader):\n",
    "    model.eval()\n",
    "    label_stack = []\n",
    "    pred_stack = torch.randn(size=(1, CFG.num_classes)).to(device)\n",
    "    for batch in loader:\n",
    "        batch = set_device(batch, device)\n",
    "        audio, label = batch\n",
    "        with torch.no_grad():\n",
    "            prob = model(audio)\n",
    "        label_stack += label.cpu().numpy().tolist()\n",
    "        pred_stack = torch.cat([pred_stack, prob], dim=0)\n",
    "    pred_stack = pred_stack[1:]\n",
    "    pred_stack = pred_stack.detach().cpu().numpy()\n",
    "    label_stack = np.array(label_stack)\n",
    "    acc, auc = measurement(label_stack, pred_stack)\n",
    "    return acc, auc\n",
    "\n",
    "model, loader_eval = load_data_model(args, df)\n",
    "acc, auc = eval(model, loader_eval)\n",
    "print(acc, auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
